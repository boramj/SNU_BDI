install.packages("ISLR") # 강의 관련 데이터를 받을 수 있는 패키지
library(ISLR)
setwd("/Users/arnorfati/Documents/GitHub/SNU_BDI/데이터마이닝")
adver <- read.csv("Advertising.csv",header=T)
head(adver)
lm.fit <- lm(Sales~., data = adver)
summary(lm.fit)
attach(adver)
lmout <- lm(Sales~TV)
summary(lmout)
est=lmout$coef
est
resid = lmout$resid
resid
anova(lm.fit)
anova(lm.fit,type='type2')
? anova
step()
credit <- read.csv("Credit.csv",header=T)
credit
setwd("C:/Users/User/Documents/GitHub/SNU_BDI/데이터마이닝")
setwd("/Users/arnorfati/Documents/GitHub/SNU_BDI/데이터마이닝")
B=100
est=matrix(0,B,3)
r=0.1
beta0=-5
beta1=1
for(k in (1:B)){
n0=10000
n1=n0*r
n=n0+n1
x=c(rep(0,n0),rep(1,n1))
eta=exp(beta0+beta1*x)
prob=eta/(1+eta)
y=rep(0,n)
for(i in (1:n)){
y[i]=rbinom(1,1,prob[i])
}
glm.fit=glm(y~x,family=binomial)
est[k,1]=sum(y)/n*100
est[k,2:3]=as.vector(glm.fit$coefficients)
cat("\n")
cat(k)
}
apply(est,2,mean)
apply(est,2,var)
?apply
beta0=-7
beta1=1
for(k in (1:B)){
n0=10000
n1=n0*r
n=n0+n1
x=c(rep(0,n0),rep(1,n1))
eta=exp(beta0+beta1*x)
prob=eta/(1+eta)
y=rep(0,n)
for(i in (1:n)){
y[i]=rbinom(1,1,prob[i])
}
glm.fit=glm(y~x,family=binomial)
est[k,1]=sum(y)/n*100
est[k,2:3]=as.vector(glm.fit$coefficients)
cat("\n")
cat(k)
}
apply(est,2,mean)
apply(est,2,var)
# 변수 선택
B=200
nums=rep(0,200)
for(k in (1:B)){
n=300
y=rbinom(n,1,0.5)
x=matrix(rnorm(n*100),n,100)
fullmod = glm(y~x,family=binomial)
pval=coef(summary(fullmod))[,4]
nums[k]=sum(pval<0.05)
}
nums
summary(nums)
# 2.2 step function
n=200
y=rbinom(n,1,0.5)
x=matrix(rnorm(n*100),n,100)
fullmod = glm(y~x,family=binomial)
summary(fullmod)
nothing <- glm(y ~ 1,family=binomial)
summary(nothing)
backwards = step(fullmod)
backwards = step(fullmod,trace=0)
formula(backwards)
summary(backwards)
forwards =step(nothing,scope=list(lower=formula(nothing),
upper=formula(fullmod)), direction="forward")
formula(forwards)
bothways =step(nothing, list(lower=formula(nothing),
upper=formula(fullmod)), direction="both",trace=0)
formula(bothways)
# 2.3 BIC 의 계산
AIC(fullmod,k=log(n))
AIC(fullmod,k=2)
AIC(fullmod)
AIC(nothing,k=log(n))
AIC(nothing,k=2)
AIC(nothing)
install.packages('glm')
? binom
? rbinom
hist(rbinom(300,1,0.5))
?rnorm
B=200 # 횟수
nums=rep(0,200)
for(k in (1:B)){
n=300
y=rbinom(n,1,0.5)
x=matrix(rnorm(n*100),n,100)
fullmod = glm(y~x,family=binomial)
pval=coef(summary(fullmod))[,4]
nums[k]=sum(pval<0.05)
}
nums
summary(nums)
n=200
y=rbinom(n,1,0.5)
x=matrix(rnorm(n*100),n,100)
fullmod = glm(y~x,family=binomial)
summary(fullmod)
nothing <- glm(y ~ 1,family=binomial)
summary(nothing)
backwards = step(fullmod)
backwards = step(fullmod,trace=0)
formula(backwards)
summary(backwards)
forwards =step(nothing,scope=list(lower=formula(nothing),
upper=formula(fullmod)), direction="forward")
formula(forwards)
bothways =step(nothing, list(lower=formula(nothing),
upper=formula(fullmod)), direction="both",trace=0)
formula(bothways)
backwards = step(fullmod)
backwards = step(fullmod,trace=0)
formula(backwards)
summary(backwards)
forwards =step(nothing,scope=list(lower=formula(nothing),
upper=formula(fullmod)), direction="forward")
library(ISLR)
library(ISLR)
attach(Smarket)
tarin = (Year<2005)
Smarket.2005=Smarket[!train,]
dim(Smarket.2005)
tarin = (Year<2005)
Smarket.2005=Smarket[!train,]
train = (Year<2005)
Smarket.2005=Smarket[!train,]
dim(Smarket.2005)
glm.fit <- glm(Direction~Lag1+Lag2+Lag3+Lag4+Lag5+volume,family = binomial,subset=train)
glm.fit <- glm(Direction~Lag1+Lag2+Lag3+Lag4+Lag5+Volume,family = binomial,subset=train)
glm.probs=predict(glm.fit,Smarket.2005,type="response")
probs=as.numeric(glm.probs)
tclass=as.numeric(Direction[!train])
c=sort(probs)
B=length(c)
roc=matrix(0,B,2)
Y-axis = Sensitivity = True positive rate
for(k in (1:B)){
dclass=as.numeric(probs>c[k])
roc[k,2]=sum((dclass==1)&(tclass==2))/sum(tclass==2)
roc[k,1]=sum((dclass==1)&(tclass==1))/sum(tclass==1)
}
plot(roc[,1],roc[,2],type="l")
plot(roc[,1],roc[,2])
abline(0,1,col="red")
0.4*100000+0.12*150000+0.48*100000
0.4*100000+0.12*130000+0.48*75000
0.4*100000+0.12*120000+0.48*75000
0.4*100000+0.12*130000+0.48*75000
glm.fit=glm(y~x,family=binomial)
est=as.numeric(glm.fit$coefficients)
peta=est[1]+tx%*%est[2:11]
m=100
ty=rbinom(n,1,0.5)
tx=matrix(rnorm(n*10),n,10)
teta=tx[,1]+tx[,2]
tprobs=exp(teta)/(1+exp(teta))
for(i in (1:n)){
ty[i]=rbinom(1,1,tprobs[i])
}
peta=est[1]+tx%*%est[2:11]
pprob=exp(peta)/(1+exp(peta))
c=sort(pprob)
B=length(c)
tclass=ty
for(k in (1:B)){
dclass=as.numeric(pprob>c[k])
roc[k,2]=sum((dclass==1)&(tclass==1))/sum(tclass==1)
roc[k,1]=sum((dclass==1)&(tclass==0))/sum(tclass==0)
}
plot(roc[,1],roc[,2],type="l")
abline(0,1,col="red")
glm.fit=glm(y~x[,1:2],family=binomial)
est=as.numeric(glm.fit$coefficients)
peta=est[1]+tx[,1:2]%*%est[2:3]
pprob=exp(peta)/(1+exp(peta))
c=sort(pprob)
B=length(c)
tclass=ty
roc=matrix(0,B,2)
for(k in (1:B)){
dclass=as.numeric(pprob>c[k])
roc[k,2]=sum((dclass==1)&(tclass==1))/sum(tclass==1)
roc[k,1]=sum((dclass==1)&(tclass==0))/sum(tclass==0)
}
plot(roc[,1],roc[,2],type="p")
abline(0,1,col="red")
glm.fit=glm(y~x[,1:2],family=binomial)
est=as.numeric(glm.fit$coefficients)
peta=est[1]+tx[,1:2]%*%est[2:3]
pprob=exp(peta)/(1+exp(peta))
c=sort(pprob)
B=length(c)
tclass=ty
roc=matrix(0,B,2)
for(k in (1:B)){
dclass=as.numeric(pprob>c[k])
roc[k,2]=sum((dclass==1)&(tclass==1))/sum(tclass==1)
roc[k,1]=sum((dclass==1)&(tclass==0))/sum(tclass==0)
}
plot(roc[,1],roc[,2],type="p")
abline(0,1,col="red")
plot(roc[,1],roc[,2],type="p")
german <- read.csv('german_numeric.csv'header=T)
german <- read.csv('german_numeric.csv',header=T)
names(german)
dim(german)
summary(german)
german_1 <- german_1[german$Result == 1,]
german_1 <- german[german$Result == 1,]
german_2 <- german[german$Result == 2,]
library(class)
dim(german)
train <- c(1:600)
test <- c(601:1000)
german[,16] <- german[,16] -1
german_0 <- german[german$Result == 0,]
german_1 <- german[german$Result == 1,]
Response <- rep(0,1000) # vector를 만들때는 우선 사이즈를 만듬, 그 뒤 각 베터에 값을 지정
for (i in c(1:10000)){
if (german[im16]==0) {Response[i]="Good"}else {Response[i]="Bad"}
}
mean(german_0[:1]) ; mean(german_1[:1])
mean(german_0[,1]) ; mean(german_1[,1])
for (i in 1:16) {
mean(german_0[,i]) ; mean(german_1[,i])
}
print(mean(german_0[,i]) ; mean(german_1[,i]))
print(mean(german_0[,i]),mean(german_1[,i]))
mean(german_0[,i]),mean(german_1[,i])
for (i in 1:16) {
mean(german_0[,i])
mean(german_1[,i])
}
attach(german)
train.x <- german[train,-16]
test.x <- german[test,--16]
train.x <- german[train,-16]
train.result <- Result[train]
test.result <- Resut[test]
test.result <- Result[test]
set.seed(1)
glm.fit=glm(Result~. data=german,family=binomial)
glm.fit=glm(Result~., data=german,family=binomial)
mean <- matrix(rep(0,32),2,16)
mean
meann <- matrix(rep(0,32),2,16)
meann
for (i in 1:16) {
meann[1,i] <- mean(german_0[,i])
meann[2,i] <- mean(german_1[,i])
}
meann
summary(glm.fit)
coef(glm.fit)
unclass(coef(glm.fit))
a <- unclass(coef(glm.fit))
a
cbind(a,meann)
length(a)
length(meann)
rbind(a,meann)
glm.probs = predic(glm.fit, type = 'response')
glm.probs = predict(glm.fit, type = 'response')
glm.probs[1:10]
glm.pred=rep('Good',1000)
glm.pred[glm.probs >.5]= 'Bad'
glm.probs
meann
glm.pred=rep('Good',1000)
glm.pred[glm.probs >.5]= 'Bad'
table(glm.pred,Response)
table(glm.pred,pred == Response)
table(glm.pred,Response)
Presponse
Response
german <- read.csv('german_numeric.csv',header=T)
train <- c(1:600)
test <- c(601:1000)
german[,16] <- german[,16] -1
Response <- rep(0,1000) # vector를 만들때는 우선 사이즈를 만듬, 그 뒤 각 베터에 값을 지정
for (i in c(1:10000)){
if (german[im16]==0) {Response[i]="Good"}else {Response[i]="Bad"}
}
meann <- matrix(rep(0,32),2,16)
attach(german)
train.x <- german[train,-16]
test.x <- german[test,--16]
train.result <- Result[train]
test.result <- Result[test]
set.seed(1)
glm.fit=glm(Result~., data=german,family=binomial)
summary(glm.fit)
coef(glm.fit)
glm.probs = predict(glm.fit, type = 'response')
glm.probs[1:10]
glm.pred=rep('Good',1000)
glm.pred[glm.probs >.5]= 'Bad'
table(glm.pred,Response)
glm.pred[glm.probs >.5]= 'Bad'
table(glm.pred,Response)
library(MASS)
lda.fit = lda(Result~., data = german,subset = train)
lda.pred = predict(lda.fit,test.x)
lda.fit = lda(Result~., data = german,subset = train)
lda.pred = predict(lda.fit,test.x)
g
attach(german)
lda.pred = predict(lda.fit,test.x)
name(lda.pred)
names(lda.pred)
lda.pred = predict(lda.fit,test.x)
library(MASS)
lda.fit = lda(Result~., data = german,subset = train)
lda.pred = predict(lda.fit,test.x)
german <- read.csv('german_numeric.csv',header=T)
names(german)
dim(german) # 자료구조, 사이즈를 알 수 있음 1000(행), 16(열,변수)
summary(german)
german_0 <- german[german$Result == 0,]
german_1 <- german[german$Result == 1,]
library(class)
train <- c(1:600)
test <- c(601:1000)
german[,16] <- german[,16] -1
Response <- rep(0,1000) # vector를 만들때는 우선 사이즈를 만듬, 그 뒤 각 베터에 값을 지정
for (i in c(1:10000)){
if (german[im16]==0) {Response[i]="Good"}else {Response[i]="Bad"}
}
meann <- matrix(rep(0,32),2,16)
attach(german)
train.x <- german[train,-16]
test.x <- german[test,--16]
train.result <- Result[train]
test.result <- Result[test]
set.seed(1)
glm.fit=glm(Result~., data=german,family=binomial)
summary(glm.fit)
coef(glm.fit)
glm.probs = predict(glm.fit, type = 'response')
glm.probs[1:10]
glm.pred=rep('Good',1000)
glm.pred[glm.probs >.5]= 'Bad'
table(glm.pred,Response)
library(MASS)
lda.fit = lda(Result~., data = german,subset = train)
lda.pred = predict(lda.fit,test.x)
lda.fit
lda.pred = predict(lda.fit,test.x)
?predict
test.x <- german[test,--16]
train.x <- german[train,-16]
train.result <- Result[train]
test.result <- Result[test]
attach(german)
train.x <- german[train,-16]
test.x <- german[test,--16]
train.result <- Result[train]
test.result <- Result[test]
set.seed(1)
glm.fit=glm(Result~., data=german,family=binomial)
summary(glm.fit)
coef(glm.fit)
glm.probs = predict(glm.fit, type = 'response')
glm.probs[1:10]
glm.pred=rep('Good',1000)
glm.pred[glm.probs > 0.5]= 'Bad'
table(glm.pred,Response)
Response
for (i in c(1:1000)){
if (german[im16]==0) {Response[i]="Good"}else {Response[i]="Bad"}
}
for (i in c(1:1000)){
if (german[im16]==0) {Response[i]="Good"}else {Response[i]="Bad"}
}
if (german[i,16]==0) {Response[i]="Good"}else {Response[i]="Bad"}
glm.pred[glm.probs > 0.5]= 'Bad'
glm.pred=rep('Good',1000)
table(glm.pred,Response)
qda.fit - qda(Result~.,data = german,subset = train)
qda.fit = qda(Result~.,data = german,subset = train)
qda.pred = predict(qda.fit,test.x)
test.x
german <- read.csv('german_numeric.csv',header=T)
names(german)
dim(german) # 자료구조, 사이즈를 알 수 있음 1000(행), 16(열,변수)
train <- c(1:600)
test <- c(601:1000)
german[,16] <- german[,16] -1
Response <- rep(0,1000) # vector를 만들때는 우선 사이즈를 만듬, 그 뒤 각 베터에 값을 지정
for (i in c(1:1000)){
if (german[i,16]==0) {Response[i]="Good"}else {Response[i]="Bad"}
}
attach(german)
train.x <- german[train,-16]
test.x <- german[test,--16]
train.result <- Result[train]
test.result <- Result[test]
set.seed(1)
glm.fit=glm(Result~., data=german,family=binomial)
summary(glm.fit)
coef(glm.fit)
glm.probs = predict(glm.fit, type = 'response')
glm.probs[1:10]
glm.pred=rep('Good',1000)
glm.pred[glm.probs > 0.5]= 'Bad'
table(glm.pred,Response)
(137+629)/1000
library(MASS)
lda.fit = lda(Result~., data = german,subset = train)
lda.fit
lda.pred = predict(lda.fit,test.x)
library(MASS)
library(MASS)
lda.fit = lda(Result~., data = german,subset = train)
lda.fit
lda.pred = predict(lda.fit,test.x)
library(MASS)
lda.fit = lda(Result ~., data = german, subset=train)
lda.fit
lda.pred = predict(lda.fit, test.x)
lda.pred
names(lda.pred)
lda.class = lda.pred$class
table(lda.class, test.result)
lda.fit = lda(Result ~., data = german, subset=train)
lda.fit
lda.pred = predict(lda.fit, test.x)
?predict
lda.pred = predict(lda.fit, newdata = test.x)
data.frame(test.x)
lda.pred = predict(lda.fit, newdata = test.x)
unclass(test.x)
german = read.csv("german_numeric.csv", header = TRUE)
names(german)
summary(german)
library(class)
dim(german)
train = c(1:600)
test = c(601:1000)
german[,16]=german[,16]-1
response = rep(0,1000)
for(i in c(1:1000)){
if (german[i,16]==0){ response[i]="good"}
else {response[i]="bad"}
}
attach(german)
train.x = german[train,-16]
test.x =german[test,-16]
train.result = Result[train]
test.result = Result[test]
set.seed(1)
glm.fit=glm(Result~.,data=german,family=binomial)
summary(glm.fit)
coef(glm.fit)
summary(glm.fit)$coef
glm.prbs=predict(glm.fit,type="response")
glm.prbs
glm.pred=rep("good",1000)
glm.pred[glm.prbs>0.5]="bad"
table(glm.pred,response)
library(MASS)
lda.fit = lda(Result ~., data = german, subset=train)
lda.fit
lda.pred = predict(lda.fit, test.x)
lda.pred
names(lda.pred)
lda.class = lda.pred$class
table(lda.class, test.result)
qda.fit = qda(Result~., data= german, subset= train)
qda.pred= predict(qda.fit, test.x)
qda.class=qda.pred$class
mean(qda.pred$class != test.result)
mean(qda.class==test.result)
table(qda.class,test.result)
train.x <- german[train,-16]
test.x <- german[test,-16]
lda.pred = predict(lda.fit, newdata = test.x)
names(lda.pred)
lda.class = lda.pred$class
table(lda.class, test.result)
lda.pred = predict(lda.fit,test.x)
1/7
